env:
  SEED: 42
  CUDA_DEVICE_ORDER: "PCI_BUS_ID" 
  OMP_NUM_THREADS: "4"
  CUDA_VISIBLE_DEVICES: "1"  # set device number here, the rest is forced to be "cuda:0"
  TRANSFORMERS_CACHE: /home/dev/public-datasets/e.shvetsov/hugging_models #/home/data/hf_cache/

paths:
  data_dir: "data/"
  log_dir: "artifacts/logs/"
  checkpoint_dir: "artifacts/checkpoints/"
  results_dir: "artifacts/results/"

model:
  path: /home/dev/public-datasets/e.shvetsov/hugging_models/gemma-3-4b-it  #/home/dev/public-datasets/e.shvetsov/hugging_models/gemma-3-4b-it #/home/dev/public-datasets/e.shvetsov/hugging_models/Qwen2.5-7B-Instruct # #/home/dev/public-datasets/e.shvetsov/hugging_models/Qwen2.5-7B-Instruct #/home/dev/public-datasets/e.shvetsov/hugging_models/Llama-3.1-8B-Instruct # #"/home/dev/public-datasets/e.shvetsov/hugging_models/Llama-2-7b-hf" # #"/home/dev/public-datasets/e.shvetsov/hugging_models/Llama-2-7b-hf"  #/home/dev/public-datasets/e.shvetsov/hugging_models/gemma-3-4b-it  # #"/home/LLM/weights/Llama-2-7b-hf"
  seqlen: 512

benchmarks:
  ppl_wikitext2:
    run_ppl: True
    batch_size: 8
  
  harness:
    run_lm_eval: False
    tasks: [arc_challenge, boolq, arc_easy, piqa, winogrande, hellaswag]  #[boolq, arc_easy, piqa, winogrande] #[arc_easy, piqa, winogrande] 
    num_fewshot: 0
    batch_size: 512
    apply_chat_template: False # for instructive models run with True and False


pruning:
    sparsity_type: semi-structured_act_magnitude #semi_structural_L_pruner # unstructured_act_magnitude, unstructured_weight_magnitude, semi-structured_act_magnitude, semi_structural_L_pruner
    transformation_type: variance #none  # none learnable
    # sparsity_ratio: 0.2 # for unstructured pruning
    additional_transformation: none
    prune_n: 2
    prune_m: 4
    module: layers #layers mlp_blocks attn_blocks
    target_modules:
        [
            # "q_proj",
            # "k_proj",
            # "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj"
        ]


finetuning:
    type: global  # by_layers, grad_acc
    output_dir: /home/LLM_activation_pruning/act_prune/artifacts/models/llama2_7b_wiki
    dataset_name: Salesforce/wikitext
    dataset_config_name: wikitext-2-raw-v1
    seed: 11
    max_seq_length: 2048
    preprocessing_num_workers: 8
    trust_remote_code: True
    num_train_epochs: 1
    # max_steps: 1000
    learning_rate: !!float 1e-4
    weight_decay: 0.0
    lr_scheduler_type: linear
    warmup_ratio:  0.03
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 16
    gradient_checkpointing: False
    save_strategy: steps
    save_steps: 200
    report_to: null
